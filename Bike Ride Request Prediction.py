# -*- coding: utf-8 -*-
"""Copy of ML_WISE_Day_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/131LcywOot3yswcm23SYn9xnikvbBUvti
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn import metrics
from sklearn.svm import SVC
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.ensemble import RandomForestRegressor

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('/content/drive/MyDrive/train.csv')
df.head()

df.shape

df = df.drop(['holiday', 'workingday','atemp'], axis=1)

df.info()

df.describe().T

"""Feature Engineering:

when multiple features are provided in the same feature or we have to derive some features from the existing ones. also try to include some extra features in our dataset

we have separated the date and time. Now let’s extract the day, month, and year from the date column.
"""

parts = df['datetime'].str.split(" ", n = 2, expand = True)
df['date'] = parts[0]
df['time'] = parts[1].str[:2].astype('int')
df.head()

parts = df['date'].str.split("-", n = 3, expand = True)
df['day'] = parts[0].astype('int')
df['month'] = parts[1].astype('int')
df['year'] = parts[2].astype('int')
df.head()

"""Based on whether it is a weekend or a weekday must have some effect on the ride request count."""

from datetime import datetime

def weekend_or_weekday(year, month, day):
  d = datetime(year, month, day)
  if d.weekday() > 4:
    return 0
  else:
    return 1

df['weekday'] = df.apply(lambda x: weekend_or_weekday(x['year'],
                                                      x['month'],
                                                      x['day']), axis = 1)

df.head()

"""Bike ride demands are also affected by whether it is am or pm."""

def am_or_pm(x):
  if x > 11:
    return 1
  else:
    return 0

df['am_or_pm'] = df['time'].apply(am_or_pm)
df.head()

"""It would be nice to have a column which can indicate whether there was any holiday on a particular day or not."""

from datetime import datetime
import holidays

def is_holiday(x):
  india_holidays = holidays.country_holidays('IN')

  if india_holidays.get(x):
    return 1
  else:
    return 0

df['holidays'] = df['date'].apply(is_holiday)
df.head()

"""remove the columns which are not useful for us."""

df.drop(['datetime', 'date'],axis = 1, inplace = True)

df

"""Exploratory Data Analysis

EDA is an approach to analyzing the data using visual techniques. It is used to discover trends, and patterns, or to check assumptions with the help of statistical summaries and graphical representations.
"""

df.isnull().sum()

print(df.columns)  # Check column names
print(df.dtypes)   # Check data types of columns

features = ['day', 'month', 'day']
plt.subplots(figsize = (15,4))
for i,col in enumerate(features):
  plt.subplot(1,3, i + 1)
  df.groupby(col).mean()['count'].plot()
plt.show()

"""There is no such pattern in the day-wise average of the ride requests.

More ride requests in the working hours as compared to the non-working hours.

The average ride request count has dropped in the month of festivals that is after the 7th month that is July that is due to more holidays in these months.
"""

features = ['season', 'weather', 'holidays', 'am_or_pm','year','weekday']

plt.subplots(figsize = (20,8))
for i, col in enumerate(features):
  plt.subplot(2,3, i+1)
  df.groupby(col).mean()['count'].plot.bar()

plt.show()

"""From the above bar plots we can confirm some real-life observations:

Ride request demand is high in the summer as well as season.

The third category was extreme weather conditions due to this people avoid taking bike rides and like to stay safe at home.

On holidays no college or offices are open due to this ride request demand is low.

More ride requests during working hours as compared to non-working hours.

Bike ride requests have increased significantly from the year 2011 to the year 2012.
"""

features = ['temp','windspeed']

plt.subplots(figsize = (15,3))
for i, col in enumerate(features):
  plt.subplot(1,2, i+1)
  sb.distplot(df[col])

plt.show()

"""Temperature values are normally distributed but due to the high number of 0 entries in the windspeed column, the data distribution shows some irregularities."""

features = ['temp', 'windspeed']

plt.subplots(figsize = (15,4))
for i, col in enumerate(features):
  plt.subplot(1,2, i +1)
  sb.boxplot(df[col])

plt.show()

"""check how much data we will lose if we remove outliers."""

num_rows = df.shape[0]- df[df['windspeed'] < 32].shape[0]
print(f'Number of rows that will be lost if we remove outliers is equal to  {num_rows}.')

"""We can remove this many rows because we have around 10000 rows of data so, this much data loss won’t affect the learning for our model."""

features = ['humidity', 'casual','registered', 'count']

plt.subplots(figsize = (15,7))
for i, col in enumerate(features):
  plt.subplot(1, 4, i+1)
  sb.boxplot(df[col])

plt.show()

"""check whether there are any highly correlated features in our dataset or not."""

sb.heatmap(df.corr() > 0.8,
           annot = True,
           cbar = False)
plt.show()

"""Here the registered feature is highly correlated with our target variable which is count.

This will lead to a situation of data leakage if we do not handle this situation.

So, let’s remove this ‘registered’ column from our feature set and also the ‘time’ feature.

Now, we have to remove the outliers we found in the above two observations that are for the humidity and wind speed.
"""

df.drop(['year'], axis = 1, inplace = True)
df = df[(df['windspeed'] < 32) & (df['humidity'] > 0)]

"""Model Training

Now we will separate the features and target variables and split them into training and the testing data by using which we will select the model which is performing best on the validation data.
"""

features = df.drop(['count'], axis = 1)
target = df['count'].values

print(features[-5:])
print(target[-5:])
X_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size = 0.2, random_state = 10)
X_train.shape, X_test.shape

"""Normalizing the data before feeding it into machine learning models helps us to achieve stable and fast training.

We have split our data into training and validation data also the normalization of the data has been done. Now let’s train some state-of-the-art machine learning models and select the best out of them using the validation dataset.
"""

from sklearn.metrics import mean_absolute_error as mae
models = [LinearRegression(), XGBRegressor(),RandomForestRegressor()]

for i in range(3):
  models[i].fit(X_train, Y_train)
  print(f'{models[i]} : ')

  train_pred = models[i].predict(X_train)
  print('Training error :' , mae(Y_train, train_pred))

  val_pred = models[i].predict(X_test)
  print('Validation error :' , mae(Y_test, val_pred))

"""The predictions made by the RandomForestRegressor are really amazing compared to the other model. In the case of RandomForestRegressor, there is a little bit of overfitting but we can manage it by hyperparameter tuning."""

model = RandomForestRegressor()
model.fit(X_train, Y_train)

import numpy
accuracy = model.score(X_test,Y_test)
print("accuracy",accuracy)

# checking for overfitting and underfitting the data
print("Training set score: {:.4f}".format(model.score(X_train,Y_train)))
print("Testing score : {:.4f}".format(model.score(X_test,Y_test)))

def predict_count(season, weather, temp, humidity, windspeed, casual, registered,time,day, month, weekday, am_or_pm, holidays):

    x = np.zeros(len(features.columns))
    x[0] = season
    x[1] = weather
    x[2] = temp
    x[3] = humidity
    x[4] = windspeed
    x[5] = casual
    x[6] = registered
    x[7] = time
    x[8] = day
    x[9] = month
    x[10] = weekday
    x[11] = am_or_pm
    x[12] = holidays

    return model.predict([x])[0]

predict_count(4,1,13.12,66,8.9981,4,84,23,19,12,1,1,0)

